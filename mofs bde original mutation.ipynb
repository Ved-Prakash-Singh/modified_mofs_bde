{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from non_dominated_sort import fast_non_dominated_sort as fnds\n",
    "from non_dominated_sort import dominance_check_between_two_points as dc\n",
    "from non_dominated_sort import insert as insert\n",
    "from non_dominated_sort import non_dominated_sorting as nds\n",
    "from fitness_calculation import calculate_error_rate as error\n",
    "from one_bit_purifying_search import one_bit_purifying_search as obps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import math\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888691d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af848774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.csv', sep=',', encoding='utf-8', index_col=False)\n",
    "df_test = pd.read_csv('test.csv', sep=',', encoding='utf-8', index_col=False)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff532480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df_test = df_test.drop('Unnamed: 0', axis=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['Creditability']\n",
    "df = df.drop(['Creditability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = df_test['Creditability']\n",
    "df_test = df_test.drop(['Creditability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee578c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006516be",
   "metadata": {},
   "source": [
    "# Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c884e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up = pd.get_dummies(df, columns = ['Account.Balance', 'Payment.Status.of.Previous.Credit', 'Purpose', 'Sex...Marital.Status', 'Guarantors', 'Most.valuable.available.asset', 'Concurrent.Credits', 'Type.of.apartment', 'Occupation', 'Foreign.Worker', 'Telephone'], drop_first= True)\n",
    "test_up = pd.get_dummies(df_test, columns = ['Account.Balance', 'Payment.Status.of.Previous.Credit', 'Purpose', 'Sex...Marital.Status', 'Guarantors', 'Most.valuable.available.asset', 'Concurrent.Credits', 'Type.of.apartment', 'Occupation', 'Foreign.Worker', 'Telephone'], drop_first= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa739c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_up.shape)\n",
    "print(test_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up_null_count = train_up.isna().sum()\n",
    "train_up_null_count = train_up_null_count[train_up_null_count>0]\n",
    "train_up_null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b47e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up = train_up.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba79951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_up_null_count = test_up.isna().sum()\n",
    "test_up_null_count = test_up_null_count[test_up_null_count>0]\n",
    "test_up_null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_up = test_up.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be73b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_up.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77c4b7",
   "metadata": {},
   "source": [
    "# Creating training test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a35d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_up.drop('target', axis=1)\n",
    "y_train = df['target']\n",
    "\n",
    "X_test = test_up.drop('target', axis=1)\n",
    "y_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec64a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d951ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3068b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_population_for_next_gen(population_size, P_t_plus_1_vector_fitness): \n",
    "    print(\"population_size\" + str(population_size))\n",
    "    print(\"len of P_t_plus_1_vector_fitness\" + str(len(P_t_plus_1_vector_fitness)))\n",
    "    P_t_plus_1 =  list(map(itemgetter(0), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = list(map(itemgetter (1,2), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = [list (item) for item in P_t_plus_1_fitness]\n",
    "    \n",
    "    sorted_index, sorted_values = fnds(data = P_t_plus_1_fitness)\n",
    "    print(\"sorted_index\")\n",
    "    print(sorted_index)\n",
    "    index_to_go_for_next_gen  = []\n",
    "    \n",
    "    for i in range(len(sorted_index)-1):\n",
    "        if (len(sorted_index[i]) <= population_size - len(index_to_go_for_next_gen)):\n",
    "            print(\"first if\")\n",
    "            print(len(sorted_index[i]))\n",
    "            print(population_size - len(index_to_go_for_next_gen))\n",
    "            index_to_go_for_next_gen = index_to_go_for_next_gen + sorted_index[i] \n",
    "        elif(len(index_to_go_for_next_gen) < population_size):\n",
    "            places_left = population_size - len(index_to_go_for_next_gen)\n",
    "            print(\"places_left\")\n",
    "            print(places_left)\n",
    "            index_to_go_for_next_gen = index_to_go_for_next_gen + sorted_index[i][0:places_left] \n",
    "        else:\n",
    "            print(\"neithr if\")\n",
    "    print(\"vedved index_to_go_for_next_gen\")\n",
    "    print(index_to_go_for_next_gen)\n",
    "    population_for_next_gen = [P_t_plus_1_vector_fitness[i] for i in index_to_go_for_next_gen] \n",
    "    print(\"loop population_for_next_gen\")\n",
    "    print(population_for_next_gen)\n",
    "    return population_for_next_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dde92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_front(P_t_plus_1_vector_fitness): \n",
    "    print(\"P_t_plus_1_vector_fitness\")\n",
    "    print(P_t_plus_1_vector_fitness)\n",
    "    P_t_plus_1 = list(map(itemgetter(0), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = list(map(itemgetter(1,2), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = [list(item) for item in P_t_plus_1_fitness]\n",
    "    \n",
    "    print(\"P_t_plus_1_fitness\")\n",
    "    print(P_t_plus_1_fitness)\n",
    "    print(\"before fnds\")\n",
    "    sorted_index, sorted_values = fnds(data = P_t_plus_1_fitness)\n",
    "    print(\"after fnds\")\n",
    "    sorted_first_front_by_error = sorted(sorted_values[0], key = lambda x: x[0], reverse=True) \n",
    "    index_of_minimum_error_element = sorted_index[0][sorted_values[0].index(sorted_first_front_by_error[0])] \n",
    "     \n",
    "    top_front = [P_t_plus_1[i] for i in sorted_index[0]]\n",
    "    optimal_sol =  P_t_plus_1[index_of_minimum_error_element]\n",
    "       \n",
    "    return top_front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b07e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_sol(P_t_plus_1_vector_fitness): \n",
    "    P_t_plus_1 = list(map(itemgetter(0), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = list(map(itemgetter(1,2), P_t_plus_1_vector_fitness))\n",
    "    P_t_plus_1_fitness = [list(item) for item in P_t_plus_1_fitness]\n",
    "    \n",
    "    sorted_index, sorted_values = fnds(data = P_t_plus_1_fitness)\n",
    "    sorted_first_front_by_error = sorted(sorted_values[0], key = lambda x: x[0], reverse=True) \n",
    "    index_of_minimum_error_element = sorted_index[0][sorted_values[0].index(sorted_first_front_by_error[0])] \n",
    "    \n",
    "    top_front = [P_t_plus_1[i] for i in sorted_index[0]]\n",
    "    optimal_sol =  P_t_plus_1[index_of_minimum_error_element]\n",
    "    optimal_sol_fitness =  P_t_plus_1_fitness[index_of_minimum_error_element]\n",
    "    \n",
    "    return optimal_sol, optimal_sol_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOFS(gen_max, obps_freq, dataset_train, target_train, dataset_test, target_test, initial_population):\n",
    "    P_0 = initial_population\n",
    "    all_gen_population_and_fitness = []\n",
    "    num_feature = 27\n",
    "    \n",
    "    for t in (range(gen_max)):\n",
    "        print(\"Attention - Generation \" +str(t+1) + \" starts\")\n",
    "        P_t_plus_1 = []\n",
    "        P_t_plus_1_vector_fitness = []\n",
    "        P_t_plus_1_vector = []\n",
    "        P_t_plus_1_fitness = []\n",
    "        round_robin_flag = 0\n",
    "        N = len(P_0)\n",
    "        print(\"NNNNNNNNNNNNNNNNNNNNNN\")\n",
    "        print(N)\n",
    "        \n",
    "        for i in range(1,N+1):\n",
    "            random.seed(t+30*i)\n",
    "            r1, r2, r3 = random.sample(range(0,N), 3)\n",
    "            print(r1, r2, r3)\n",
    "            print(P_0[r1],P_0[r2],P_0[r3])\n",
    "            X_r1, num_feature_r1, f1_score_r1 = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = P_0[r1])\n",
    "            X_r2, num_feature_r2, f1_score_r2 = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = P_0[r2])\n",
    "            X_r3, num_feature_r3, f1_score_r3 = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = P_0[r3])\n",
    "            \n",
    "            Dict_vector_cardinality_accuracy = {(num_feature_r1, f1_score_r1):X_r1, (num_feature_r2, f1_score_r2):X_r2, (num_feature_r3, f1_score_r3):X_r3 }\n",
    "            num_feature_and_accuracy_for_3_candidate_sol = [[num_feature_r1, f1_score_r1], [num_feature_r2, f1_score_r2], [num_feature_r3, f1_score_r3]]\n",
    "            \n",
    "            res, smallest_key = nds(num_feature_and_accuracy_for_3_candidate_sol)\n",
    "            res_smallest_key = res[smallest_key]\n",
    "            \n",
    "            res_smallest_key = sorted(res_smallest_key, key=lambda x:x[1], reverse = False)\n",
    "            round_robin_flag = round_robin_flag + 1\n",
    "            \n",
    "            if(res_smallest_key[0] + res_smallest_key[0] != 0):\n",
    "                X_best_t = res_smallest_key[0]\n",
    "            else:\n",
    "                X_best_t = res_smallest_key[1]\n",
    "            \n",
    "            removed_ = Dict_vector_cardinality_accuracy[tuple(X_best_t)]\n",
    "            \n",
    "            Dict_vector_cardinality_accuracy_after_removing_best = Dict_vector_cardinality_accuracy.copy()\n",
    "            Dict_vector_cardinality_accuracy_after_removing_best.pop(tuple(X_best_t))\n",
    "            \n",
    "            list_of_Dict_vector_cardinality_accuracy_after_removing_best = list(Dict_vector_cardinality_accuracy_after_removing_best.items())\n",
    "            \n",
    "            if(len(list_of_Dict_vector_cardinality_accuracy_after_removing_best) == 0):\n",
    "                X_r1_new = Dict_vector_cardinality_accuracy[tuple(X_best_t)]\n",
    "                X_r2_new = Dict_vector_cardinality_accuracy[tuple(X_best_t)]\n",
    "            elif(len(list_of_Dict_vector_cardinality_accuracy_after_removing_best) == 1):\n",
    "                X_r1_new = list_of_Dict_vector_cardinality_accuracy_after_removing_best[0][1]\n",
    "                X_r2_new = list_of_Dict_vector_cardinality_accuracy_after_removing_best[0][1]\n",
    "            else:\n",
    "                X_r1_new = list_of_Dict_vector_cardinality_accuracy_after_removing_best[0][1]\n",
    "                X_r2_new = list_of_Dict_vector_cardinality_accuracy_after_removing_best[1][1]\n",
    "                \n",
    "            base_vector = Dict_vector_cardinality_accuracy[tuple(X_best_t)]\n",
    "            X_i = P_0[i-1]\n",
    "            \n",
    "            X_ri, num_feature_ri, f1_score_ri = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = X_i)\n",
    "            X_rbase, num_feature_rbase, f1_score_rbase = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = base_vector)\n",
    "            \n",
    "            xi_dominates_base, base_dominates_xi, base_and_xi_are_nondominating = dc(point1=[num_feature_rbase, f1_score_rbase], point2 =[num_feature_ri, f1_score_ri])\n",
    "            \n",
    "            X_r1_for_xor = X_r1_new.copy()\n",
    "            X_r2_for_xor = X_r2_new.copy()\n",
    "            \n",
    "            sigma = 0.01\n",
    "            c_i = [0]*num_feature\n",
    "            v_i = [0]*num_feature\n",
    "            F=0.1\n",
    "            random.seed(t+30*i)\n",
    "            random_val = random.sample(range(0,100), 1)\n",
    "            random_value = random_val[0]/100\n",
    "            \n",
    "            if(base_dominates_xi == True):\n",
    "                c_i = [sigma]*num_feature\n",
    "            else:\n",
    "                for i in range(num_feature):\n",
    "                    c_i[i] = min(1, (F*int((bool(X_r1_for_xor[i]) ^ bool(X_r2_for_xor[i])))+sigma))\n",
    "            mu = math.exp(-math.sqrt(t)) #t is generation count\n",
    "            \n",
    "            #for i in range(num_feature):\n",
    "            #    v_i[i] = mu * (math.floor((X_r1[i] + X_r2[i] + X_r3[i])/1.5)) + (1-mu) *((base_vector[i] * int(bool(X_r1_new[i]) ^ bool(X_r2_new[i]))) + (int(not(bool(base_vector[i]))) * int(bool(X_r1_new[i]) & bool(X_r2_new[i]))))\n",
    "            #    if(v_i[i] < 0.5):\n",
    "            #        v_i[i] = 0\n",
    "            #    else:\n",
    "            #        v_i[i] = 1\n",
    "                    \n",
    "            for i in range(num_feature):\n",
    "                if c_i[i] < random_value:\n",
    "                    v_i[i] = base_vector[i]\n",
    "                else:\n",
    "                    v_i[i] = 1-base_vector[i]\n",
    "                \n",
    "            if sum(v_i)>0:\n",
    "                pass\n",
    "            else:\n",
    "                v_i = base_vector\n",
    "                \n",
    "            random.seed(t+30*i)\n",
    "\n",
    "            u_0_1 = random.sample(range(0,100), 1)[0]\n",
    "            u_0_1 = u_0_1/100\n",
    "\n",
    "            random.seed(t+30*i)\n",
    "            h=random.sample(range(0,N),1)[0]\n",
    "            cr=0.5\n",
    "            u_i = [0]*num_feature\n",
    "                \n",
    "            for i in range(num_feature):\n",
    "                if(u_0_1 < cr or i==h):\n",
    "                    u_i[i] = v_i[i]\n",
    "                else:\n",
    "                    u_i[i] = X_i[i]\n",
    "                        \n",
    "            X_ui, num_feature_ui, f1_score_ui = error(dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, X_rn = u_i)\n",
    "            xi_dominates_ui, ui_dominates_xi, ui_and_xi_are_nondominating = dc(point1=[num_feature_ui, f1_score_ui], point2 =[num_feature_ri, f1_score_ri])\n",
    "\n",
    "            if(xi_dominates_ui == 1):\n",
    "                print(\"xi_dominates_ui\")\n",
    "                P_t_plus_1_vector_fitness.append([X_ri, num_feature_ri, f1_score_ri])\n",
    "            elif(ui_dominates_xi == 1):\n",
    "                print(\"ui_dominates_xi\")\n",
    "                P_t_plus_1_vector_fitness.append([X_ui, num_feature_ui, f1_score_ui])\n",
    "            else:\n",
    "                print(\"xi ui nondominating\")\n",
    "                P_t_plus_1_vector_fitness.append([X_ri, num_feature_ri, f1_score_ri])\n",
    "                P_t_plus_1_vector_fitness.append([X_ui, num_feature_ui, f1_score_ui])\n",
    "        \n",
    "        print(\"P_t_plus_1_vector_fitnessP_t_plus_1_vector_fitnessP_t_plus_1_vector_fitnessP_t_plus_1_vector_fitness\")\n",
    "        print(P_t_plus_1_vector_fitness)\n",
    "        \n",
    "        #Remove duplicates\n",
    "        P_t_plus_1_vector_fitness_unique=[]\n",
    "        for i in P_t_plus_1_vector_fitness:\n",
    "            if i not in P_t_plus_1_vector_fitness_unique:\n",
    "                P_t_plus_1_vector_fitness_unique.append(i)\n",
    "                \n",
    "        dup_count = len(P_t_plus_1_vector_fitness) - len(P_t_plus_1_vector_fitness_unique)\n",
    "        #print(dup_count)\n",
    "        \n",
    "        print(\"P_t_plus_1_vector_fitness\")\n",
    "        print(P_t_plus_1_vector_fitness)\n",
    "        print(\"P_t_plus_1_vector_fitness_unique\")\n",
    "        print(P_t_plus_1_vector_fitness_unique)\n",
    "        \n",
    "        P_t_plus_1_vector_fitness = P_t_plus_1_vector_fitness_unique.copy()\n",
    "        \n",
    "        if(gen_max/(obps_freq+1) == math.ceil(gen_max/(obps_freq+1))):\n",
    "            top_front_ = top_front(P_t_plus_1_vector_fitness=P_t_plus_1_vector_fitness)\n",
    "            print(\"top_front_\")\n",
    "            print(top_front_)\n",
    "            optimal_sol_, optimal_sol_fitness_ = optimal_sol(P_t_plus_1_vector_fitness=P_t_plus_1_vector_fitness)\n",
    "            obps_result = obps(X_train, X_test, y_train, y_test, S=top_front_, optimal_sol = optimal_sol_)\n",
    "                        \n",
    "            to_be_removed_sol = optimal_sol_.copy()\n",
    "            to_be_removed_fit = optimal_sol_fitness_.copy()\n",
    "                       \n",
    "            to_be_removed_sol = [to_be_removed_sol, to_be_removed_fit[0], to_be_removed_fit[1]]\n",
    "            \n",
    "            P_t_plus_1_vector_fitness.remove(to_be_removed_sol)\n",
    "            P_t_plus_1_vector_fitness.append(obps_result[0])\n",
    "        print(\"vedP_t_plus_1_vector_fitness\")\n",
    "        print(P_t_plus_1_vector_fitness)\n",
    "        print(\"len of P_t_plus_1_vector_fitness\")\n",
    "        print(len(P_t_plus_1_vector_fitness))\n",
    "        print(N)\n",
    "        if len(P_t_plus_1_vector_fitness) > N:\n",
    "            population_for_next_gen_ = select_population_for_next_gen(population_size=N, P_t_plus_1_vector_fitness=P_t_plus_1_vector_fitness)\n",
    "        else:\n",
    "            population_for_next_gen_ = P_t_plus_1_vector_fitness\n",
    "        \n",
    "        print(\"population_for_next_gen_population_for_next_gen_\")\n",
    "        print(population_for_next_gen_)\n",
    "        all_gen_population_and_fitness.append(population_for_next_gen_)\n",
    "        \n",
    "        P_0 = list(map(itemgetter(0), population_for_next_gen_))\n",
    "        \n",
    "    return P_t_plus_1_vector_fitness, all_gen_population_and_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "Feature_population = []\n",
    "rng = np.random.default_rng(27) #number of features\n",
    "for i in range(100): #number of candidate solution in initial population\n",
    "    array_of_0_1 = rng.random(27) #number of features\n",
    "    array_of_0_1[array_of_0_1>=0.5] = 1\n",
    "    array_of_0_1[array_of_0_1<0.5] = 0\n",
    "    array_of_0_1=array_of_0_1.astype(int)\n",
    "    list_of_0_1 = list(array_of_0_1)\n",
    "    \n",
    "    Feature_population.append(list_of_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653478c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Feature_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360928e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Feature_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_gen_vector_fitness, all_generation_population_and_fitness = MOFS(gen_max=200, obps_freq=1, dataset_train=X_train, target_train = y_train, dataset_test = X_test, target_test = y_test, initial_population = Feature_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_generation_population_and_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668630ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
